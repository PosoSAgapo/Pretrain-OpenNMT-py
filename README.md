# Pretrain-OpenNMT-py: Open-Source Neural Machine Translation in Pretrain Version

This branch is a extension from OpenNMT-py which supports a pre-train model including BERT model or other pre-trained models. The target of this branch is to make OpenNMT a research friendly project that support pre-train model, auto evaluation and find the best checkpoint on the test set.
Before you use this package, you should refer to [OpenNMT](https://github.com/OpenNMT/OpenNMT-py) for basic usage as this package works as extension. However, as this is an independent extension for OpenNMT, so I may not be able to main this package to update with OpenNMT's new release, but I will try my best.
## Completed Features:
  ### BERT as Encoder
  ### BERT as Embedding
 
 ## Future Project:
  ### Generation Pre-trained Model
  ### Parrallel Training and Inference
 
 ## Others
 
 This project will be organized and re-publish as another package since OpenNMT does not consider to include pre-trained models.
 
Feel free to send a PR or feature request, I will reply at my best.
